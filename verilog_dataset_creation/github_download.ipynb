{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from github import Github\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "ACCESS_TOKEN = 'ADD YOUR TOKEN WITH REPO ACCESS RIGHTS HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_extension(path):\n",
    "    return path.rsplit(\".\",1)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_github_request_checker():\n",
    "    rate_data = g.get_rate_limit().core.raw_data\n",
    "    if rate_data['remaining'] < 50:\n",
    "        time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "        print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "        time.sleep(time_to_reset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [\n",
    "    \"id\",\n",
    "    \"clone_url\",\n",
    "    \"created_at\",\n",
    "    \"description\",\n",
    "    \"full_name\",\n",
    "    \"language\",\n",
    "    \"name\",\n",
    "    \"size\",\n",
    "    \"stargazers_count\",\n",
    "    \"updated_at\",\n",
    "    \"forks_count\"\n",
    "]\n",
    "\n",
    "data_funcs_list = [\"get_topics\",\"get_license\"]\n",
    "\n",
    "data_list.sort()\n",
    "\n",
    "df = pd.DataFrame(columns=data_list + [\"topics\",\"license_url\"])\n",
    "\n",
    "def add_repo_to_df(df, repo):\n",
    "    data = [getattr(repo,attr) for attr in data_list]\n",
    "    data.append(repo.get_topics())\n",
    "    try:\n",
    "        license_url = repo.get_license().license.url\n",
    "    except:\n",
    "        license_url = \"None\"\n",
    "    data.append(license_url)\n",
    "    df.loc[len(df)] = data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_github(language, start_date, end_date):\n",
    "    \"\"\"\n",
    "    More info:\n",
    "    https://docs.github.com/en/search-github/getting-started-with-searching-on-github/understanding-the-search-syntax#query-for-dates\n",
    "    \"\"\"\n",
    "    assert(language == \"VHDL\" or language == 'Verilog' or language == 'SystemVerilog')\n",
    "    date_q = f\"{start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}\" \n",
    "    result = g.search_repositories(\"\",language='Verilog',created=date_q) \n",
    "    print(f\"Found {result.totalCount} repos for: {language}, {date_q}\")\n",
    "    return result\n",
    "\n",
    "def process_repo_search_results(df,results):\n",
    "    for i in range(1000):\n",
    "\n",
    "        rate_data = g.get_rate_limit().core.raw_data\n",
    "        now_seconds = int(time.time())\n",
    "        if rate_data['remaining'] < 100:\n",
    "            time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "            print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "            time.sleep(time_to_reset)\n",
    "        page = results.get_page(i)\n",
    "        page_size = len(page)\n",
    "        for j in range(page_size):\n",
    "            df = add_repo_to_df(df,page[j])\n",
    "        if page_size < 30:\n",
    "            break   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repos(df, language, start_date, end_date):\n",
    "    repo_search_results = search_github(language, start_date, end_date)\n",
    "    if repo_search_results.totalCount > 0:\n",
    "        if repo_search_results.totalCount == 1000:\n",
    "            # Reduce date range (recursively?)\n",
    "            delta = (end_date - start_date) / 2\n",
    "            df = find_repos(df, language,start_date, end_date - delta)\n",
    "            df = find_repos(df, language, start_date + delta, end_date)\n",
    "        else:\n",
    "            df = process_repo_search_results(df,repo_search_results)\n",
    "    print(f\"Done: {start_date.strftime('%Y-%m-%d')}..{end_date.strftime('%Y-%m-%d')}, df length: {len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=data_list + [\"topics\",\"license_url\"])\n",
    "language = \"Verilog\"\n",
    "# You can split up the search to make it more manageable by splitting your search over certain years\n",
    "start_date = datetime(1980,1,1)\n",
    "end_date = datetime.now()\n",
    "print(df)\n",
    "df = find_repos(df, language, start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51321"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{language}_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=data_list + [\"topics\",\"license_url\"])\n",
    "language = 'SystemVerilog'\n",
    "start_date = datetime(1980,1,1)\n",
    "end_date = datetime.now()\n",
    "print(df2)\n",
    "df2 = find_repos(df2, language, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16258"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(f\"{language}_{start_date.strftime('%Y-%m-%d')}_{end_date.strftime('%Y-%m-%d')}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding licenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_deduplicate_gh_search_results(csvs):\n",
    "    df = pd.concat(map(lambda x: pd.read_csv(x,na_values=['None']), csvs), ignore_index=True)\n",
    "    df = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df = df.drop_duplicates([c for c in df.columns if c != 'updated_at'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_df = combine_and_deduplicate_gh_search_results([os.path.relpath('./data/search_repo_indices/Verilog_1980-01-01_2013-01-01.csv'), os.path.relpath('./data/search_repo_indices/Verilog_1980-01-01_2022-10-12_16576.csv'),os.path.relpath('./data/search_repo_indices/Verilog_2018-12-08_2022-10-13.csv')])\n",
    "systemverilog_df = combine_and_deduplicate_gh_search_results([os.path.relpath('./data/search_repo_indices/SystemVerilog_1980-01-01_2022-10-15_part.csv'),os.path.relpath('./data/search_repo_indices/SystemVerilog_2021-05-14_2022-10-15_part2.csv')])\n",
    "\n",
    "print(len(verilog_df))\n",
    "print(len(systemverilog_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_indices_dir = 'data/search_repo_indices'\n",
    "verilog_df.to_csv(os.path.join(repo_indices_dir,\"full_verilog_repos.csv\"))\n",
    "systemverilog_df.to_csv(os.path.join(repo_indices_dir,\"full_systemverilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_licenses_from_repo_df_to_dict(repo_df,licenses_dict):\n",
    "    df_with_unique_licenses = repo_df.loc[repo_df['license_url'].dropna().drop_duplicates().index]\n",
    "    repo_ids = list(df_with_unique_licenses['id'])\n",
    "    for rid in repo_ids:\n",
    "        pre_github_request_checker()\n",
    "        license_data = g.get_repo(rid).get_license().license.raw_data\n",
    "        licenses_dict[license_data['url']] = license_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "licenses_dict = {}\n",
    "add_licenses_from_repo_df_to_dict(verilog_df,licenses_dict)\n",
    "add_licenses_from_repo_df_to_dict(systemverilog_df,licenses_dict)\n",
    "df = pd.DataFrame.from_dict(licenses_dict,orient='index')\n",
    "df.to_csv(os.path.join('data/search_repo_indices','licenses.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_licenses_with_permissions_conditions(license_df,permissions=[],conditions=[]):\n",
    "    indices = []\n",
    "    for i, row in license_df.iterrows():\n",
    "        if len(permissions) == 0 or set(permissions).issubset(set(row['permissions'])):\n",
    "            if len(conditions) == 0 or len(set(conditions).intersection(set(row['conditions']))) > 0 :\n",
    "                indices.append(i)\n",
    "    return license_df.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "permissions = ['modifications','distribution']\n",
    "special_conditions = ['same-license--file','same-license--library','same-license']\n",
    "permissive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=permissions,conditions=[])\n",
    "distributive_licenses_df = get_licenses_with_permissions_conditions(df,permissions=['distribution'],conditions=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(permissive_licenses_df))\n",
    "print(len(distributive_licenses_df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion, all licenses found are permissive in that they allow modifications and distribution! Repos without licenses are not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sv_df = systemverilog_df.dropna(subset=['license_url'])\n",
    "p_ve_df = verilog_df.dropna(subset=['license_url'])\n",
    "\n",
    "p_sv_df.to_csv(os.path.join(repo_indices_dir,\"permissive_systemverilog_repos.csv\"))\n",
    "p_ve_df.to_csv(os.path.join(repo_indices_dir,\"permissive_verilog_repos.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([verilog_df,systemverilog_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "p_all_df = pd.concat([p_ve_df,p_sv_df]).drop_duplicates(subset=[c for c in verilog_df.columns if not c in ['language']])\n",
    "\n",
    "all_df.to_csv(\"all_deduplicated_repos.csv\")\n",
    "p_all_df.to_csv(\"permissive_all_deduplicated_repos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50171\n",
      "7516\n"
     ]
    }
   ],
   "source": [
    "print(len(all_df))\n",
    "print(len(p_all_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "all_permissions = []\n",
    "all_conditions = []\n",
    "all_limitations = []\n",
    "for i,row in df.iterrows():\n",
    "    all_permissions.append(row['permissions'])\n",
    "    all_conditions.append(row['conditions'])\n",
    "    all_limitations.append(row['limitations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'disclose-source', 'same-license'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " ['include-copyright',\n",
       "  'disclose-source',\n",
       "  'document-changes',\n",
       "  'same-license--library'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " ['include-copyright',\n",
       "  'document-changes',\n",
       "  'disclose-source',\n",
       "  'network-use-disclose',\n",
       "  'same-license'],\n",
       " [],\n",
       " ['include-copyright'],\n",
       " [],\n",
       " ['disclose-source', 'include-copyright', 'same-license--file'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['include-copyright', 'document-changes', 'same-license'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright--source'],\n",
       " ['include-copyright--source', 'document-changes'],\n",
       " ['include-copyright', 'document-changes'],\n",
       " ['disclose-source', 'include-copyright', 'same-license'],\n",
       " ['include-copyright']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clone_command(clone_url,directory, depth=1,branch='master'):\n",
    "    command_parts = [f'git clone']\n",
    "    # command_parts.append(f\"-b {branch}\") # TODO: rerun search without master branch (causes error for some searches!)\n",
    "    command_parts.append(f\"--depth {depth}\")\n",
    "    command_parts.append(f\"--no-tags\")\n",
    "    # command_parts.append(f\"--no-checkout\")\n",
    "    command_parts.append(clone_url)\n",
    "    command_parts.append(directory)\n",
    "    return \" \".join(command_parts)\n",
    "\n",
    "def create_clone_script_for_df(repo_df,script_out_path,clone_out_dir):\n",
    "    with open(script_out_path,'w+') as f:\n",
    "        for i,row in repo_df.iterrows():\n",
    "            clone_url = row['clone_url']\n",
    "            out_dir = os.path.join(clone_out_dir,str(row['id']))\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.mkdir(out_dir)\n",
    "            f.write(create_clone_command(clone_url,str(os.path.abspath(out_dir))).replace(\"\\\\\",\"/\") + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_clone_script_for_df(p_all_df,\"./clone_all_p.sh\",\"data/full_repos/permissive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'git clone -b master --depth 1 --no-tags --no-checkout https://github.com/mrehkopf/sd2snes.git ./data/full_repos/exp'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_clone_command(\"https://github.com/mrehkopf/sd2snes.git\",\"./data/full_repos/exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verilog_df = pd.concat([p_ve_df,p_sv_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7291\n"
     ]
    }
   ],
   "source": [
    "print(len(p_sv_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14537\n",
      "7478\n"
     ]
    }
   ],
   "source": [
    "print(len(all_verilog_df))\n",
    "print(len(all_verilog_df.drop_duplicates(subset=[c for c in all_verilog_df.columns if not c in ['language']])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_files_without_right_extension(start_dir, extensions_to_keep):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir):\n",
    "        for file in [sf for sf in files if not get_file_extension(sf) in extensions_to_keep]:\n",
    "            file_path = os.path.join(root,file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except Exception as e:\n",
    "                errors.append(e)\n",
    "    return errors\n",
    "\n",
    "def delete_empty_dirs(start_dir):\n",
    "    errors = []\n",
    "    for root, dirs, files in os.walk(start_dir,topdown=False):\n",
    "        for d in dirs:\n",
    "            dir_path = os.path.join(root,d)\n",
    "            if len(os.listdir(dir_path)) == 0:\n",
    "                try:\n",
    "                    os.rmdir(dir_path)\n",
    "                except Exception as e:\n",
    "                    errors.append(e)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.intel.com/content/www/us/en/programmable/quartushelp/17.0/reference/glossary/glosslist.htm\n",
    "# https://marketplace.visualstudio.com/items?itemName=eirikpre.systemverilog\n",
    "verilog_extension_files = ['v','verilog','vlg','vh']\n",
    "system_verilog_extension_files = ['sv','svh','svp']\n",
    "extra_file_types = ['vo','vt'] # verilog output, verilog test bench\n",
    "\n",
    "extensions_to_keep = verilog_extension_files + system_verilog_extension_files + extra_file_types\n",
    "\n",
    "start_dir = os.path.relpath(\"data/full_repos/permissive\")\n",
    "\n",
    "files_errors = delete_all_files_without_right_extension(start_dir,extensions_to_keep)\n",
    "dirs_errors = delete_empty_dirs(start_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process github searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_deduplicate_gh_search_results(csvs):\n",
    "    df = pd.concat(map(pd.read_csv, csvs), ignore_index=True)\n",
    "    df = df.drop(['Unnamed: 0'],axis=1)\n",
    "    df = df.drop_duplicates([c for c in df.columns if c != 'updated_at'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49166\n",
      "49277\n",
      "49250\n"
     ]
    }
   ],
   "source": [
    "verilog_df = combine_and_deduplicate_gh_search_results([os.path.relpath('./data/search_repo_indices/Verilog_1980-01-01_2013-01-01.csv'), os.path.relpath('./data/search_repo_indices/Verilog_1980-01-01_2022-10-12_16576.csv'),os.path.relpath('./data/search_repo_indices/Verilog_2018-12-08_2022-10-13.csv')])\n",
    "systemverilog_df = combine_and_deduplicate_gh_search_results([os.path.relpath('./data/search_repo_indices/SystemVerilog_1980-01-01_2022-10-15_part.csv'),os.path.relpath('./data/search_repo_indices/SystemVerilog_2021-05-14_2022-10-15_part2.csv')])\n",
    "\n",
    "print(len(verilog_df))\n",
    "print(len(systemverilog_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global file_count\n",
    "file_count = 0\n",
    "\n",
    "global files_dict\n",
    "files_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_github_request_checker():\n",
    "    rate_data = g.get_rate_limit().core.raw_data\n",
    "    if rate_data['remaining'] < 50:\n",
    "        time_to_reset = rate_data['reset'] - int(time.time()) + 1\n",
    "        print(f\"Sleeping for {time_to_reset} seconds\")\n",
    "        time.sleep(time_to_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_files_dict(count, content):\n",
    "    global files_dict\n",
    "    files_dict[count] = {\n",
    "        \"path\": content.raw_data['path'],\n",
    "        \"size\": content.raw_data['size'],\n",
    "        \"count_id\": count\n",
    "    }\n",
    "\n",
    "def download_files_from_repo(repo,extensions,out_dir):\n",
    "    global files_dict\n",
    "    global file_count\n",
    "    file_count = 0\n",
    "    files_dict = {}\n",
    "    pre_github_request_checker()\n",
    "    contents = repo.get_contents(\"/\")\n",
    "    for content in contents:\n",
    "        download_content(repo,content,extensions,out_dir)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(files_dict,orient='index')\n",
    "    print(f\"Saving csv index with {len(df)} entries\")\n",
    "    df.to_csv(os.path.join(out_dir,\"index.csv\"))\n",
    "    \n",
    "def download_content(repo,content,extensions,out_dir):\n",
    "    content_raw_data = content.raw_data\n",
    "    content_type = content_raw_data['type']\n",
    "    if content_type == 'dir':\n",
    "        pre_github_request_checker()\n",
    "        new_contents = repo.get_contents(content_raw_data['path'])\n",
    "        for new_content in new_contents:\n",
    "            download_content(repo,new_content,extensions,out_dir)\n",
    "    elif content_type == 'file':\n",
    "        extension = get_file_extension(content_raw_data['name'])\n",
    "        if extension in extensions:\n",
    "            global file_count\n",
    "            update_files_dict(file_count,content)\n",
    "            pre_github_request_checker()\n",
    "            try:\n",
    "                with open(os.path.join(out_dir,str(file_count) + \".\" + extension),'wb') as f:\n",
    "                    f.write(content.decoded_content)\n",
    "                file_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Caught exception while trying to write content:\\n{e}\")\n",
    "    # raise Exception(f\"Content type not recognized: {content_type}\")\n",
    "\n",
    "def download_all_repos(df,extensions,out_dir):\n",
    "    all_repo_ids = list(df['id'])\n",
    "    for i in range(14,len(df['id'])):\n",
    "        repo_id = all_repo_ids[i]\n",
    "        pre_github_request_checker()\n",
    "        repo = g.get_repo(repo_id)\n",
    "        repo_dir = os.path.join(out_dir,str(repo_id))\n",
    "        if not os.path.exists(repo_dir):\n",
    "            os.makedirs(repo_dir)\n",
    "        print(f\"Searching repo {i} with id: {repo_id}\")\n",
    "        download_files_from_repo(repo,extensions,repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching repo 14 with id: 4519428\n",
      "Caught exception while trying to write content:\n",
      "unsupported encoding: none\n",
      "Sleeping for 2158 seconds\n",
      "Saving csv index with 983 entries\n",
      "Searching repo 15 with id: 753580\n",
      "Sleeping for 1946 seconds\n"
     ]
    }
   ],
   "source": [
    "download_all_repos(verilog_df,verilog_extension_files,os.path.relpath(\"data/repos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_files_df(downloaded_repo_dir,extensions_to_keep):\n",
    "    extensions_map = {ext: True for ext in extensions_to_keep}\n",
    "    df = pd.DataFrame(columns=['directory','repo_id','file_name','extension'])\n",
    "    for repo_id in os.listdir(downloaded_repo_dir):\n",
    "        for root,dirs,files in os.walk(os.path.join(downloaded_repo_dir,repo_id)):\n",
    "            for file in files:\n",
    "                extension = get_file_extension(file)\n",
    "                try:\n",
    "                    if extensions_map[extension]:\n",
    "                        directory = os.path.join(root,file)\n",
    "                        df.loc[len(df)] = [directory, repo_id, file, extension]\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    extensions_map[extension] = False\n",
    "        print(f\"Done with repo: {repo_id}\")\n",
    "    print(f\"Extensions: {extensions_map}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df = create_files_df('data/full_repos/permissive',verilog_extension_files + system_verilog_extension_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "verilog_files_df.to_csv('./files_index.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition dataset for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = pd.read_csv('data/search_repo_indices/files_index.csv',index_col=0)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314877"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verilog_extension_files = ['v','verilog','vlg','vh']\n",
    "system_verilog_extension_files = ['sv','svh','svp']\n",
    "\n",
    "files_index = files_index[files_index['extension'].isin(verilog_extension_files + system_verilog_extension_files)]\n",
    "# files_index = files_index.reset_index(drop=True)\n",
    "len(files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index = files_index.reset_index(drop=True)\n",
    "# files_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_indices = np.random.choice(len(files_index),replace=False,size=200)\n",
    "remaining_files_index = files_index.drop(index=few_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314677"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_files_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_partitions = 10\n",
    "tot_len = len(remaining_files_index)\n",
    "for i in range(number_of_partitions):\n",
    "    partition_df = remaining_files_index.iloc[list(range(i*tot_len//number_of_partitions,(i+1)*tot_len//number_of_partitions))]\n",
    "    partition_df.to_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_len = len(remaining_files_index)\n",
    "total = 0\n",
    "for i in range(number_of_partitions):\n",
    "    length = len(pd.read_csv(f\"data/verilog_partitions/files_index_part_{i}.csv\"))\n",
    "    total += length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill partitions with source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_source_code(directory):\n",
    "    # Biggest error = utf-8 encoding problem\n",
    "    try:\n",
    "        # return open(directory,'r').read()\n",
    "        with codecs.open(directory,encoding='utf-8', errors='replace', mode = 'r') as f:\n",
    "            data = f.read()\n",
    "        return data.replace(\"\\x00\",\"\") # replacing this might not be needed but someone online said it helps...\n",
    "    except Exception as e:\n",
    "        e_string = f\"0:FOUND ERROR: {e}\"\n",
    "        print(e_string)\n",
    "        return e_string\n",
    "\n",
    "def clean_row_directory(row):\n",
    "    return row['directory'].replace(\"\\\\\",\"/\")\n",
    "\n",
    "def add_source_code_to_index_df(df):\n",
    "    df['directory'] = df.apply(lambda row: clean_row_directory(row),axis=1)\n",
    "    df['code'] = \"\"\n",
    "    tqdm.pandas(desc='Apply read_source_code')\n",
    "    df['code'] = df.progress_apply(lambda row: read_source_code(row['directory']),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code:  94%|█████████▎| 29496/31468 [05:13<00:28, 70.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apply read_source_code: 100%|██████████| 31468/31468 [05:48<00:00, 90.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "number_of_partitions = 10\n",
    "for i in range(number_of_partitions):\n",
    "    df_dir = f\"data/verilog_partitions/files_index_part_{i}.csv\"\n",
    "    print(f\"Starting {i}\")\n",
    "    partition_df = pd.read_csv(df_dir,index_col=0)\n",
    "    new_partition_df = add_source_code_to_index_df(partition_df)\n",
    "    new_partition_df.to_csv(df_dir)\n",
    "    del partition_df, new_partition_df\n",
    "print(\"All done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hdl_dataset_creation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d483179aadbe36b266083fb168142eacd02134ef8f8b2756794bec1efb632f92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
